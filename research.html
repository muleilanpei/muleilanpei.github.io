<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Research Projects</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Muleilan PEI</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="background.html">Background</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="publication.html">Publication</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research Projects</h1>
</div>
<h2>An Improved Dyna-Q Algorithm for Mobile Robot Path Planning in Unknown Dynamic Environment </h2>
<table class="imgtable"><tr><td>
<img src="sim_dynamic.gif" alt="alt text" width="300px" height="300px" />&nbsp;</td>
<td align="left"><p><b>Abstract</b>:This paper deals with the problem of mobile robot path planning in an unknown environment that contains both static and dynamic obstacles, utilizing a reinforcement learning approach. The proposed method, namely the improved Dyna-Q algorithm, incorporates heuristic search strategies, simulated annealing mechanism and reactive navigation principle into Q-learning based on the Dyna architecture. A novel action-selection strategy combining &epsilon-greedy policy with cooling schedule control is presented, which, together with the heuristic reward function and heuristic actions, can tackle the exploration-exploitation dilemma and enhance the performance in global searching, convergence property and learning efficiency for path planning. The proposal is validated to be superior to the classical Q-learning and Dyna-Q algorithms in an unknown static environment, and successfully applied to an uncertain environment with multiple dynamic obstacles in simulations. Moreover, practical experiments are conducted by integrating MATLAB and ROS on a physical robot platform and the mobile robot manages to find a collision-free path, thus fulfilling the autonomous navigation tasks in real world.  <br />
[Under review]</p>
</td></tr></table>
<h2>Quadruped Robot Locomotion in Unknown Terrain Using Deep Reinforcement Learning</h2>
<table class="imgtable"><tr><td>
<img src="prob.png" alt="alt text" width="300px" height="210px" />&nbsp;</td>
<td align="left"><p><b>Abstract</b>:This paper is concerned with locomotion problems for the quadruped robot in unknown and unstructured terrains, utilizing the emerging deep reinforcement learning technique. The state-of-the-art deep deterministic policy gradient (DDPG) algorithm is leveraged to acquire the gait policy by learning from interactions with the environment. The framework of learning system is presented based on the actor-critic architecture, and the additional domain-specific knowledge is exploited for shaping the reward function to enhance the learning efficiency during training. Moreover, a simulation study is implemented to validate the performance of the proposed DDPG-based controller.<br />
[<a href="https://ieeexplore.ieee.org/abstract/document/9274920">Paper</a>]</p>
</td></tr></table>
</td>
</tr>
</table>
</body>
</html>
